{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94dbcda8-e12a-4848-a07c-b99ca94dec98",
   "metadata": {},
   "source": [
    "# Driving a Racecar with Reinforcement Learning\n",
    "---\n",
    "The aim of this notebook is to train a reinforcement learning model to learn how to navigate the Box2D Car Racing environment, provided by the Gymnasium online project <a href=\"https://gymnasium.farama.org/environments/box2d/car_racing/\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6513c97-908b-4a2b-9f3c-8c8cd1959cd0",
   "metadata": {},
   "source": [
    "## Problem setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2680388-e329-4dc5-a1b9-7fa0fc86ee75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "environment_name = \"CarRacing-v2\"\n",
    "env = gym.make(environment_name, render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab678cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates track in a separate window\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893bc36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# racing track, 96 x 96 image with 3 colour overlays\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963facc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces new window with racetrack environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close opened track environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fc8c0-cc4a-4f22-a9b8-c424fe00ee43",
   "metadata": {},
   "source": [
    "You can observe the car's path when taking random actions by running the below code. An episode will terminate when either a fixed number of timesteps have passed, the car visits all the track tiles, or the car falls off the racetrack (in which case it receives reward -100). Feel free to change the number of `episodes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e719b-17c2-426a-b600-c6fdd0952dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "environment_name = \"CarRacing-v2\"\n",
    "env = gym.make(environment_name, render_mode='human')\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        # random action\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info, _ = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the environment if still open in a separate window\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f43a330-ad0c-4997-8a77-a55c0680508e",
   "metadata": {},
   "source": [
    "## Model training\n",
    "---\n",
    "We will train our racecar using the Proximal Policy Optimisation (PPO) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19d386-db29-4e59-851f-3a622fdc9eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f505e6-2b4a-4bbf-be1d-fd393d2c8621",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa1c1e",
   "metadata": {},
   "source": [
    "The `CnnPolicy` policy network is able to deal with image recognition, which is how our agent observes the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f7d89-2d8d-4d5f-ab55-3622543b34df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# multi-layer perceptron policy\n",
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89595097",
   "metadata": {},
   "source": [
    "One can modify the number of epochs in which the model is trained. The results can subsequently be saved and later evaluated, viewed, etc. When training, we can view some standard metrics from the model, such as the loss and timesteps elapsed. By including the `tensorboard_log=log_path` parameter in the above code block, we are able to dump the training metrics onto our local machine for visuals in Tensorboard later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b52d04-19a9-44cc-b6fb-277353d5ff1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps = 100000\n",
    "\n",
    "# train the model\n",
    "model.learn(total_timesteps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6db7a-fe60-4316-bea5-ea323e80fc6c",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c407f2-e43f-4be5-9a1d-1ffe3c078cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_racecar_1,000,000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312c3d7-4218-429b-947b-cbaa10738f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model to specified location\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b1f05-e63b-46a4-a27e-e27fd7f8af78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete and reload model\n",
    "# del model\n",
    "# model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd413eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ae3fc-ca66-434c-8e69-ff2697c89e88",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd70b3",
   "metadata": {},
   "source": [
    "We can view the performance metrics of our model thanks to the `tensorboard_log` from earlier. To view the board, open the command prompt and activate a virtual environment which has TensorFlow installed. Then run the line `tensorboard --logdir=` followed immediately by the file directory where the Tensorboard log from training was saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51397597",
   "metadata": {},
   "source": [
    "We can also used the saved model zip file to view our trained models in action. To so do, run the first three code blocks below. Once the runs have completed, you can then reset the simulation by running the third code block, then the first two again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_racecar_1,000,000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca03004",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CarRacing-v2\"\n",
    "env = gym.make(environment_name, render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8112f97-71cc-4fa1-82d7-ab55a5706658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env=env)\n",
    "evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c9915-f43c-433b-89be-9fa715f343f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "gymenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
